{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "import string\n",
    "import re\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sent = sentence.lower()\n",
    "    sent = sent.strip()\n",
    "    sent = \"<start> \" + sent + \" <end>\"\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "with open('./data/train.en','r',encoding=\"utf8\") as f:\n",
    "    rawData = f.read().strip().split(\"\\n\")\n",
    "for line in rawData:\n",
    "    line = preprocess(line)\n",
    "    X_train.append(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/tst2013.en','r',encoding=\"utf8\") as f:\n",
    "    rawData = f.read().strip().split(\"\\n\")\n",
    "for line in rawData:\n",
    "    line = preprocess(line)\n",
    "    X_test.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = []\n",
    "Y_test = []\n",
    "with open('./data/train.vi','r',encoding=\"utf8\") as f:\n",
    "    rawData = f.read().strip().split(\"\\n\")\n",
    "for line in rawData:\n",
    "    line = preprocess(line)\n",
    "    Y_train.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/tst2013.vi','r',encoding=\"utf8\") as f:\n",
    "    rawData = f.read().strip().split(\"\\n\")\n",
    "for line in rawData:\n",
    "    line = preprocess(line)\n",
    "    Y_test.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_obj = Tokenizer()\n",
    "total_reviews = X_train + X_test\n",
    "tokenizer_obj.fit_on_texts(total_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lenght = max([len(s.split()) for s in total_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vocal_size = len(tokenizer_obj.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = tokenizer_obj.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer_obj.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train_tokens,maxlen = max_lenght, padding = 'post')\n",
    "X_test_pad = pad_sequences(X_test_tokens,maxlen = max_lenght, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_obj = Tokenizer()\n",
    "total_reviews = Y_train + Y_test\n",
    "tokenizer_obj.fit_on_texts(total_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lenght = max([len(s.split()) for s in total_reviews])\n",
    "Y_vocal_size = len(tokenizer_obj.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_tokens = tokenizer_obj.texts_to_sequences(Y_train)\n",
    "Y_test_tokens = tokenizer_obj.texts_to_sequences(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pad = pad_sequences(Y_train_tokens,maxlen = max_lenght, padding = 'post')\n",
    "Y_test_pad = pad_sequences(Y_test_tokens,maxlen = max_lenght, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4166\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = X_train_pad.shape[0]\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "hidden_unit = 1024\n",
    "embedding_size = 256\n",
    "print(N_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X_train_pad, Y_train_pad))\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\hai nam\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "(32, 630)\n",
      "(32, 852)\n"
     ]
    }
   ],
   "source": [
    "tmp_x, tmp_y = next(iter(dataset))\n",
    "print(tmp_x.shape)\n",
    "print(tmp_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encode(tf.keras.Model):\n",
    "    def __init__(self, embedding_size, vocab_size, hidden_units):\n",
    "        super(Encode, self).__init__()\n",
    "        self.Embedding = tf.keras.layers.Embedding(vocab_size,embedding_size)\n",
    "        self.GRU = tf.keras.layers.GRU(\n",
    "            hidden_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform')\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "    def call(self, x, hidden_state):\n",
    "        try:\n",
    "            x = self.Embedding(x)\n",
    "        except:\n",
    "            print(x, print(inp_lang.vocab_size))          \n",
    "        outputs, last_state = self.GRU(x, hidden_state)\n",
    "        return outputs, last_state\n",
    "    \n",
    "    def init_hidden_state(self, batch_size):\n",
    "        return tf.zeros([batch_size, self.hidden_units])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 630, 1024)\n",
      "(32, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encode(embedding_size, X_vocal_size, hidden_unit)\n",
    "hidden_state = encoder.init_hidden_state(BATCH_SIZE)\n",
    "tmp_outputs, last_state = encoder(tmp_x, hidden_state)\n",
    "print(tmp_outputs.shape)\n",
    "print(last_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, hidden_units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W_out_encode = tf.keras.layers.Dense(hidden_unit)\n",
    "        self.W_state = tf.keras.layers.Dense(hidden_unit)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, encode_outs, pre_state):\n",
    "        pre_state = tf.expand_dims(pre_state, axis=1)\n",
    "        pre_state = self.W_state(pre_state)\n",
    "        encode_outs = self.W_out_encode(encode_outs)\n",
    "        score = self.V(\n",
    "            tf.nn.tanh(\n",
    "                pre_state + encode_outs)\n",
    "        )\n",
    "        score = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = score*encode_outs\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1024) (32, 630, 1)\n"
     ]
    }
   ],
   "source": [
    "attention = Attention(hidden_unit)\n",
    "context_vector, attention_weight = attention(tmp_outputs, last_state)\n",
    "print(context_vector.shape, attention_weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1024) (32, 630, 1024) (32,)\n"
     ]
    }
   ],
   "source": [
    "class Decode(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_units):\n",
    "        super(Decode, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.Embedding = tf.keras.layers.Embedding(vocab_size,embedding_size)\n",
    "        self.Attention = Attention(hidden_units)\n",
    "        self.GRU = tf.keras.layers.GRU(\n",
    "            hidden_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "        self.Fc = tf.keras.layers.Dense(vocab_size)\n",
    "            \n",
    "    def call(self, x, encode_outs, pre_state):\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        try:\n",
    "            x = self.Embedding(x)\n",
    "        except:\n",
    "            print(x, print(tar_lang.vocab_size))          \n",
    "        context_vector, attention_weight = self.Attention(encode_outs, pre_state)\n",
    "        context_vector = tf.expand_dims(context_vector, axis=1)\n",
    "        gru_inp = tf.concat([x, context_vector], axis=-1)\n",
    "        out_gru, state = self.GRU(gru_inp)\n",
    "        out_gru = tf.reshape(out_gru, (-1, out_gru.shape[2]))\n",
    "        return self.Fc(out_gru), state\n",
    "    \n",
    "    \n",
    "decode = Decode(Y_vocal_size, embedding_size, hidden_unit)\n",
    "print(last_state.shape, tmp_outputs.shape, tmp_y[:, 0].shape)\n",
    "decode_out, state = decode(tmp_y[:, 0], tmp_outputs, last_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EPOCHS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-89d5996a403b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_BATCH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EPOCHS' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch_id, (x, y) in enumerate(dataset.take(N_BATCH)):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            first_state = encoder.init_hidden_state(batch_size=BATCH_SIZE)\n",
    "            encode_outs, last_state = encoder(x, first_state)\n",
    "            decode_state = last_state\n",
    "            decode_input = [tar_lang.word2id[\"<start>\"]]*BATCH_SIZE\n",
    "            \n",
    "            for i in range(1, y.shape[1]):\n",
    "                decode_out, decode_state = decoder(decode_input, encode_outs, decode_state)\n",
    "                loss += loss_function(y[:, i], decode_out)\n",
    "                decode_input = y[:, i]\n",
    "                \n",
    "            train_vars = encoder.trainable_variables + decoder.trainable_variables\n",
    "            grads = tape.gradient(loss, train_vars)\n",
    "            optimizer.apply_gradients(zip(grads, train_vars))\n",
    "        total_loss += loss\n",
    "    print(total_loss.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
